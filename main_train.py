import torch
import torch.optim as optim
import argparse
import numpy as np
from sklearn.metrics.pairwise import cosine_similarity
import torch.nn.functional as F

# æœ¬åœ°æ¨¡å—å¯¼å…¥
from data_loader import load_and_prepare_data
from model import NodeEmbeddingModel

def get_args():
    parser = argparse.ArgumentParser(description="Advanced ESA Model Training with InfoNCE Loss")

    # --- æ•°æ®ç›¸å…³å‚æ•° ---
    data_group = parser.add_argument_group('Data Parameters')
    data_group.add_argument('--data_path', type=str, default='datasets/Douban_0.2.npz',help='è¾“å…¥æ•°æ®æ–‡ä»¶çš„è·¯å¾„ (.npzæ ¼å¼)')
    data_group.add_argument('--split_ratios', type=float, nargs=3, default=[0.15, 0.05, 0.8],
                            help='è®­ç»ƒé›†ã€éªŒè¯é›†ã€æµ‹è¯•é›†çš„æ¯”ä¾‹ã€‚')
    data_group.add_argument('--seed', type=int, default=42, help='ç”¨äºæ•°æ®åˆ’åˆ†å’Œåˆå§‹åŒ–çš„éšæœºç§å­')

    # --- æ¨¡å‹æ¶æ„å‚æ•° ---
    model_group = parser.add_argument_group('Model Architecture')
    model_group.add_argument('--embed_dim', type=int, default=128,help='æœ€ç»ˆè¾“å‡ºçš„èŠ‚ç‚¹åµŒå…¥ç»´åº¦')
    model_group.add_argument('--hidden_dims', type=int, nargs='+', default=[64, 64, 128],help='æ¯ä¸ªæ³¨æ„åŠ›å±‚çš„éšè—ç»´åº¦åˆ—è¡¨')
    model_group.add_argument('--num_heads', type=int, nargs='+', default=[4, 4, 8],help='æ¯ä¸ªæ³¨æ„åŠ›å±‚çš„å¤´æ•°åˆ—è¡¨')
    model_group.add_argument('--layer_types', type=str, nargs='+', default=['M', 'M', 'S'],
                             choices=['M', 'S'],
                             help="æ³¨æ„åŠ›å±‚çš„ç±»å‹åºåˆ— ('M' for Masked, 'S' for Standard)")
    model_group.add_argument('--no_mlp', action='store_true',help='å¦‚æœè®¾ç½®ï¼Œåˆ™ä¸åœ¨æ³¨æ„åŠ›å—åä½¿ç”¨MLP')
    model_group.add_argument('--dropout', type=float, default=0.1,help='æ¨¡å‹ä¸­ä½¿ç”¨çš„dropoutç‡')
    model_group.add_argument('--use_pca', action='store_true' ,help='æ˜¯å¦å¯¹è¾“å…¥ç‰¹å¾ä½¿ç”¨PCAé™ç»´')
    model_group.add_argument('--pca_dim', type=int, default=128,help='PCAé™ç»´åçš„ç›®æ ‡ç»´åº¦')

    # --- è®­ç»ƒè¿‡ç¨‹å‚æ•° ---
    train_group = parser.add_argument_group('Training Parameters')
    train_group.add_argument('--lr', type=float, default=0.0005, help='å­¦ä¹ ç‡')
    train_group.add_argument('--weight_decay', type=float, default=1e-5, help='æƒé‡è¡°å‡')
    train_group.add_argument('--epochs', type=int, default=500, help='è®­ç»ƒè½®æ•°')
    train_group.add_argument('--eval_interval', type=int, default=10, help='è¯„ä¼°é—´éš”')
    train_group.add_argument('--neg_sample_k', type=int, default=128, # InfoNCE é€šå¸¸å—ç›Šäºæ›´å¤šçš„è´Ÿæ ·æœ¬
                             help='æ¯ä¸ªæ­£æ ·æœ¬å¯¹ä½¿ç”¨çš„è´Ÿæ ·æœ¬æ•°é‡')
    train_group.add_argument('--temperature', type=float, default=0.1,
                             help='InfoNCE loss ä¸­çš„æ¸©åº¦ç³»æ•°')

    # --- è¿è¡Œè®¾å¤‡å‚æ•° ---
    run_group = parser.add_argument_group('Runtime Parameters')
    run_group.add_argument('--device', type=str, default='auto',help="device ('auto', 'cpu', 'cuda')")

    args = parser.parse_args()
    
    # åå¤„ç†ä¸æ ¡éªŒ
    if args.device == 'auto':
        args.device = 'cuda' if torch.cuda.is_available() else 'cpu'
    # å…¶ä»–æ ¡éªŒ...
    return args

def get_negative_samples(pos_pairs, num_nodes_net2, k, device):
    """
    ä¸ºæ¯ä¸ªæ­£æ ·æœ¬å¯¹éšæœºé‡‡æ ·kä¸ªè´Ÿæ ·æœ¬ã€‚
    InfoNCEä¸ä¸€å®šéœ€è¦å›°éš¾è´Ÿé‡‡æ ·ï¼Œéšæœºé‡‡æ ·é€šå¸¸æ•ˆæœå·²ç»å¾ˆå¥½ï¼Œä¸”æ•ˆç‡æ›´é«˜ã€‚
    """
    pos_src = pos_pairs[:, 0].unsqueeze(1) # [N, 1]
    neg_candidates = torch.randint(0, num_nodes_net2, (pos_pairs.shape[0], k), device=device) # [N, K]
    
    # ç®€å•åœ°å°†æºèŠ‚ç‚¹ä¸éšæœºè´Ÿæ ·æœ¬ç»„åˆ
    neg_pairs_src = pos_src.expand(-1, k).flatten() # [N*K]
    neg_pairs_tgt = neg_candidates.flatten() # [N*K]
    
    return neg_pairs_src, neg_pairs_tgt


def info_nce_loss(pos_sim, neg_sim, temperature):
    """
    è®¡ç®— InfoNCE Lossã€‚
    pos_sim: [N], æ­£æ ·æœ¬å¯¹çš„ç›¸ä¼¼åº¦
    neg_sim: [N, K], Nä¸ªæºèŠ‚ç‚¹åˆ†åˆ«ä¸Kä¸ªè´Ÿæ ·æœ¬çš„ç›¸ä¼¼åº¦
    """
    # å°†æ­£æ ·æœ¬ç›¸ä¼¼åº¦ä¹ŸåŠ å…¥logitsä¸­
    # logits å½¢çŠ¶: [N, 1+K]
    logits = torch.cat([pos_sim.unsqueeze(1), neg_sim], dim=1)
    logits /= temperature
    
    # æ ‡ç­¾æ˜¯ç¬¬ä¸€åˆ— (æ­£æ ·æœ¬)
    labels = torch.zeros(logits.shape[0], dtype=torch.long, device=logits.device)
    
    return F.cross_entropy(logits, labels)


@torch.no_grad()
def evaluate(model, data, split='val'):
    # è¯„ä¼°å‡½æ•°ä¿æŒä¸å˜
    model.eval()
    all_embeddings = model(data['features'], data['edge_index'], data['batch_mapping'])
    embeds1 = all_embeddings[:data['num_nodes_net1']]
    embeds2 = all_embeddings[data['num_nodes_net1']:]

    pairs_to_eval = data[f'{split}_pairs']
    source_embeds = embeds1[pairs_to_eval[:, 0]]
    sim_matrix = cosine_similarity(source_embeds.cpu().numpy(), embeds2.cpu().numpy())

    ranks = []
    for i in range(len(pairs_to_eval)):
        true_target_id = pairs_to_eval[i, 1].item()
        scores = sim_matrix[i, :]
        sorted_indices = np.argsort(-scores)
        rank = np.where(sorted_indices == true_target_id)[0][0]
        ranks.append(rank + 1)
        
    ranks = np.array(ranks)
    mrr, h1, h10 = np.mean(1.0 / ranks), np.mean(ranks <= 1), np.mean(ranks <= 10)
    return mrr, h1, h10

def main(args):
    data = load_and_prepare_data(args)

    model = NodeEmbeddingModel(
        in_features=data['features'].shape[1],
        embed_dim=args.embed_dim,
        hidden_dims=args.hidden_dims,
        num_heads=args.num_heads,
        layer_types=args.layer_types,
        use_mlp=not args.no_mlp,
        dropout=args.dropout
    ).to(args.device)

    optimizer = optim.AdamW(model.parameters(), lr=args.lr, weight_decay=args.weight_decay)
    
    best_mrr = 0.0
    best_model_state = None

    print("\n--- å¼€å§‹è®­ç»ƒ (InfoNCE Loss) ---")
    for epoch in range(args.epochs):
        model.train()
        optimizer.zero_grad()
        
        all_embeddings = model(data['features'], data['edge_index'], data['batch_mapping'])
        embeds1 = all_embeddings[:data['num_nodes_net1']]
        embeds2 = all_embeddings[data['num_nodes_net1']:]

        pos_pairs = data['train_pairs']
        
        # --- æŸå¤±è®¡ç®—æµç¨‹ä¿®æ”¹ ---
        pos_src_embeds = embeds1[pos_pairs[:, 0]]
        pos_tgt_embeds = embeds2[pos_pairs[:, 1]]
        
        # 1. è®¡ç®—æ­£æ ·æœ¬å¯¹çš„ä½™å¼¦ç›¸ä¼¼åº¦
        pos_sim = torch.sum(F.normalize(pos_src_embeds) * F.normalize(pos_tgt_embeds), dim=1)

        # 2. è·å–è´Ÿæ ·æœ¬
        neg_src_idx, neg_tgt_idx = get_negative_samples(pos_pairs, data['num_nodes_net2'], args.neg_sample_k, args.device)
        neg_src_embeds = embeds1[neg_src_idx]
        neg_tgt_embeds = embeds2[neg_tgt_idx]

        # 3. è®¡ç®—è´Ÿæ ·æœ¬å¯¹çš„ä½™å¼¦ç›¸ä¼¼åº¦
        neg_sim_flat = torch.sum(F.normalize(neg_src_embeds) * F.normalize(neg_tgt_embeds), dim=1)
        # å°†å…¶é‡å¡‘ä¸º [N, K] çš„å½¢çŠ¶ï¼Œä»¥ä¾¿æ¯ä¸ªæ­£æ ·æœ¬éƒ½æœ‰Kä¸ªå¯¹åº”çš„è´Ÿæ ·æœ¬ç›¸ä¼¼åº¦
        neg_sim = neg_sim_flat.view(pos_pairs.shape[0], args.neg_sample_k)
        
        # 4. è®¡ç®— InfoNCE Loss
        loss = info_nce_loss(pos_sim, neg_sim, args.temperature)

        loss.backward()
        optimizer.step()

        if (epoch + 1) % args.eval_interval == 0 or epoch == args.epochs - 1:
            mrr, h1, h10 = evaluate(model, data, split='val')
            print(f"Epoch {epoch+1:03d} | Loss: {loss.item():.4f} | Val MRR: {mrr:.4f} | Val H@1: {h1:.4f} | Val H@10: {h10:.4f}")
            
            if mrr > best_mrr:
                best_mrr = mrr
                best_model_state = model.state_dict()
                print(f"ğŸš€ New best model found at epoch {epoch+1} with MRR: {best_mrr:.4f}")

    print("\nè®­ç»ƒå®Œæˆï¼")

    if best_model_state:
        print("\n--- åŠ è½½æœ€ä½³æ¨¡å‹å¹¶åœ¨æµ‹è¯•é›†ä¸Šè¿›è¡Œæœ€ç»ˆè¯„ä¼° ---")
        model.load_state_dict(best_model_state)
        test_mrr, test_h1, test_h10 = evaluate(model, data, split='test')
        print("\n--- The final result ---")
        print(f"  - Test MRR:     {test_mrr:.4f}")
        print(f"  - Test Hits@1:  {test_h1:.4f}")
        print(f"  - Test Hits@10: {test_h10:.4f}")
    else:
        print("\næœªèƒ½æ‰¾åˆ°æœ€ä½³æ¨¡å‹ï¼Œè·³è¿‡æœ€ç»ˆæµ‹è¯•ã€‚")

if __name__ == '__main__':
    args = get_args()
    main(args)